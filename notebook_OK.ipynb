{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SiVf_YXeJBj"
   },
   "source": [
    "# Project: Strategic document monitoring from https://www.theses.fr\n",
    "\n",
    "## Website presentation\n",
    "\n",
    "The website [theses.fr](https://www.theses.fr/) centralize all thesis in France from 1985. This is why having a strategic document monitoring tool is necessary to stay informed about theses on our subject.\n",
    "\n",
    "This notebook is the tool to make surveillance from [theses.fr](https://www.theses.fr/). \n",
    "\n",
    "## How to use this tool ?\n",
    "Each time you run this notebook :  \n",
    "1. the notebook will ask to you keywords/researches to search,\n",
    "  - you can write multiple researches if you separate them with ';'. </br>\n",
    "  <u>example</u>: <i>immunology; infectious diseases</i> correspond to 2 differents researches. Results to these 2 researches will be aggregate\n",
    "\n",
    "2.  The notebook will extract informations and return to you a file will link, title and abstract of found thesis.\n",
    "\n",
    "If you want to make weekly surveillance, to need to run this python tool each week, and each time use the same keywords/researches. Results from precedent extraction will not be presented.\n",
    "\n",
    "If you run this notebook with differents keywords/researches, this will be consider as new surveillance, and results from precedent extraction will be presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import openpyxl\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "from tkinter import messagebox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HG2zrz7QfYCB"
   },
   "source": [
    "# Legal aspects\n",
    "\n",
    "The website [theses.fr](https://www.theses.fr/) forbid to scrap some thesis. These thesis are listed here : [https://www.theses.fr/robots.txt](https://www.theses.fr/robots.txt).\n",
    "\n",
    "## Load robots.txt from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1683488634805,
     "user": {
      "displayName": "Pierre-Etienne TOULEMONDE",
      "userId": "12485771126098027481"
     },
     "user_tz": -120
    },
    "id": "TfxXncKLiuAv",
    "outputId": "6f78c4f9-b097-472f-d22a-615e22e98f95",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdvd270\\AppData\\Local\\Temp\\ipykernel_29596\\3086229146.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  robots_df = pd.read_csv(\"https://www.theses.fr/robots.txt\", sep = \": \").rename(columns = {\"User-agent\":\"col\", \"*\":\"id_thesis\"})\n"
     ]
    }
   ],
   "source": [
    "# Load robots.txt file\n",
    "robots_df = pd.read_csv(\"https://www.theses.fr/robots.txt\", sep = \": \").rename(columns = {\"User-agent\":\"col\", \"*\":\"id_thesis\"})\n",
    "# display(robots_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract illegal URL from robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1683488634808,
     "user": {
      "displayName": "Pierre-Etienne TOULEMONDE",
      "userId": "12485771126098027481"
     },
     "user_tz": -120
    },
    "id": "fuSEBBknceRZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "robots_df = robots_df[ (robots_df[\"col\"] != \"Crawl-delay\") & (robots_df[\"col\"] != \"Sitemap\") ] # Delete Site map and Crawl-delay rows\n",
    "\n",
    "illegal_url_list = robots_df.id_thesis.apply(lambda x: \"https://www.theses.fr\"+x).tolist() # List of disallow URL\n",
    "\n",
    "# robots_df.to_csv(\"illegal_urls.csv\")\n",
    "# display(illegal_url_list) # Check : OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB5xWCyTceRc"
   },
   "source": [
    "# App\n",
    "\n",
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "error",
     "timestamp": 1683488638532,
     "user": {
      "displayName": "Pierre-Etienne TOULEMONDE",
      "userId": "12485771126098027481"
     },
     "user_tz": -120
    },
    "id": "cI2V41O8ceRg",
    "outputId": "ce1c0012-d564-4980-a864-9e96adac2b60",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw user input : 'test ; pathologie digitale ; transformation chimique'\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"\n",
    "result = False\n",
    "\n",
    "while user_input == \"\" : \n",
    "    # User input\n",
    "    tk_window = tk.Tk()\n",
    "    tk_window.geometry(\"150x150\")\n",
    "\n",
    "    tk_window.withdraw()\n",
    "    # the input dialog\n",
    "    user_input = simpledialog.askstring(title=\"Request\",\n",
    "                                      prompt=\"Please select your keywords : \\n \"+\n",
    "                                        \"(you cam make multiples research in the same time if you separate them with semicolon (;) )\")\n",
    "\n",
    "    # User confirmation\n",
    "    tk_window.geometry(\"150x150\")\n",
    "    result = messagebox.askyesno(\"Request confirmation\", \"Your request is : \" + user_input) # Renvoie True si oui, False si non\n",
    " \n",
    "print(\"Raw user input : '\" + user_input + \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1683487688543,
     "user": {
      "displayName": "Pierre-Etienne TOULEMONDE",
      "userId": "12485771126098027481"
     },
     "user_tz": -120
    },
    "id": "KsF-iJvJh2qK"
   },
   "outputs": [],
   "source": [
    "# user_input = \"      prout    dsfdsqf    ;      tesqfdsq   fdsqfdsqf   efdsq;           \" # user-input for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nQzlt35ceRj"
   },
   "source": [
    "## Parsing user request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "error",
     "timestamp": 1683488640207,
     "user": {
      "displayName": "Pierre-Etienne TOULEMONDE",
      "userId": "12485771126098027481"
     },
     "user_tz": -120
    },
    "id": "rRZpVGcxceRn",
    "outputId": "8867273b-0880-4afe-b82d-b304e337e4bd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element n° 1  on  3\n",
      "element : 'test'\n",
      "element n° 2  on  3\n",
      "element : 'pathologie+digitale'\n",
      "element n° 3  on  3\n",
      "element : 'transformation+chimique'\n",
      "Final list of elements:  ['test', 'pathologie+digitale', 'transformation+chimique']\n"
     ]
    }
   ],
   "source": [
    "# print(user_input)\n",
    "# user_input = \"    prîut    dsfdèqf;   tésqfdsq fdsùfdàqf      efééééésq   ;                \" # user_input for testing\n",
    "\n",
    "input_list = user_input.split(\";\")\n",
    "mask = []\n",
    "\n",
    "for count, element in enumerate(input_list) : \n",
    "    print(\"element n°\", count+1, \" on \", len(input_list))\n",
    "    element = re.sub(' +', ' ', element) # Delete multiple spaces\n",
    "    element = re.sub('^ +', '', element) # Delete spaces before reseach\n",
    "    element = re.sub(' +$', '', element) # Delete spaces after reseach\n",
    "    element = re.sub(' ', '+', element)\n",
    "    element = unidecode(element) # delete accent\n",
    "    \n",
    "    print(\"element : '\" + element + \"'\")\n",
    "    \n",
    "    if len(element) == 0 :\n",
    "        print(\"1 element deleted because containing nothing\")\n",
    "    else : \n",
    "        mask = mask + [element]\n",
    "\n",
    "input_list = mask\n",
    "\n",
    "# Verifying output\n",
    "# print(\"Liste finale : \")\n",
    "print(\"Final list of elements: \",str(input_list))\n",
    "\n",
    "if len(input_list) == 0 :\n",
    "  sys.exit(\"No input user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5P1YXI7gceRy",
    "tags": []
   },
   "source": [
    "## Website site extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1683489180687,
     "user": {
      "displayName": "Pierre-Etienne TOULEMONDE",
      "userId": "12485771126098027481"
     },
     "user_tz": -120
    },
    "id": "iXHPpWmN5046"
   },
   "outputs": [],
   "source": [
    "# Function to extract results.\n",
    "# theses.fr allow to export only 1000 results in the same file, so it's necessary to make multiple exports and stack them.\n",
    "\n",
    "def scraping_number_results(url_short) :\n",
    "  url_short = url_short\n",
    "\n",
    "  # Number of results\n",
    "  html = requests.get(url_short)\n",
    "  soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "  number_results = int(soup.find(\"div\", attrs={\"id\":\"resumR\"}).find(\"span\", attrs={\"id\":\"sNbRes\"}).text)\n",
    "\n",
    "  return number_results\n",
    "\n",
    "def result_scraping(n_res, url) :\n",
    "  url = url\n",
    "  number_results = n_res\n",
    "  \n",
    "  # Variables\n",
    "  df_temp = pd.DataFrame()\n",
    "  definitive_df  = pd.DataFrame()\n",
    "\n",
    "  # Loop extraction\n",
    "  start = 0\n",
    "  number_results_loop = number_results\n",
    "\n",
    "  while number_results_loop >= 0 :\n",
    "    number_results_loop -= 1000\n",
    "    # print(\"url : \" + str(url.format(start)))\n",
    "    print(\"For element : \" + element + \", extraction from \" + str(start) + \" to \" + str(min(number_results, start+1000)))\n",
    "    df_temp = pd.read_csv(url.format(start), sep = \";\")\n",
    "\n",
    "    definitive_df = pd.concat([definitive_df, df_temp], ignore_index = True)\n",
    "    \n",
    "    start += 1000\n",
    "\n",
    "  # inital : df = pd.read_csv(url)\n",
    "  # pb : si plus de 1000 résultats, csv ne charge que les 1000 premeirs résultats\n",
    "\n",
    "  return definitive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14781,
     "status": "ok",
     "timestamp": 1683489199851,
     "user": {
      "displayName": "Pierre-Etienne TOULEMONDE",
      "userId": "12485771126098027481"
     },
     "user_tz": -120
    },
    "id": "r4yKyTIJceR0",
    "outputId": "3ce95649-7b33-4e0a-a1cf-91c76af3d9e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------\n",
      "Element : transformation+chimique\n",
      "File from previous researches found.\n",
      "\n",
      "Number of thesis already seen in preceent researches : 20905\n",
      "20905  results for element :  transformation+chimique\n",
      "For element : transformation+chimique, extraction from 0 to 1000\n",
      "For element : transformation+chimique, extraction from 1000 to 2000\n",
      "For element : transformation+chimique, extraction from 2000 to 3000\n",
      "For element : transformation+chimique, extraction from 3000 to 4000\n",
      "For element : transformation+chimique, extraction from 4000 to 5000\n",
      "For element : transformation+chimique, extraction from 5000 to 6000\n",
      "For element : transformation+chimique, extraction from 6000 to 7000\n",
      "For element : transformation+chimique, extraction from 7000 to 8000\n",
      "For element : transformation+chimique, extraction from 8000 to 9000\n",
      "For element : transformation+chimique, extraction from 9000 to 10000\n",
      "For element : transformation+chimique, extraction from 10000 to 11000\n",
      "For element : transformation+chimique, extraction from 11000 to 12000\n",
      "For element : transformation+chimique, extraction from 12000 to 13000\n",
      "For element : transformation+chimique, extraction from 13000 to 14000\n",
      "For element : transformation+chimique, extraction from 14000 to 15000\n",
      "For element : transformation+chimique, extraction from 15000 to 16000\n",
      "For element : transformation+chimique, extraction from 16000 to 17000\n",
      "For element : transformation+chimique, extraction from 17000 to 18000\n",
      "For element : transformation+chimique, extraction from 18000 to 19000\n",
      "For element : transformation+chimique, extraction from 19000 to 20000\n",
      "For element : transformation+chimique, extraction from 20000 to 20905\n",
      "Thesis already seen in precedent researches : 20905\n",
      "duplicate : 0\n",
      "Disallow thesis : 0\n",
      "Extraction from transformation+chimique finished! \n",
      "\n",
      "-------------------------------------------------\n",
      "Element : pathologie+digestive+numerique\n",
      "File from previous researches found.\n",
      "\n",
      "Number of thesis already seen in preceent researches : 224\n",
      "224  results for element :  pathologie+digestive+numerique\n",
      "For element : pathologie+digestive+numerique, extraction from 0 to 224\n",
      "Thesis already seen in precedent researches : 224\n",
      "duplicate : 0\n",
      "Disallow thesis : 0\n",
      "Extraction from pathologie+digestive+numerique finished! \n",
      "\n",
      "---------------------------\n",
      "Execution finished !\n"
     ]
    }
   ],
   "source": [
    "# input_list = [\"transformation+chimique\", \"pathologie+digestive+numerique\"] # input list for testing\n",
    "\n",
    "thesis_df_temp = []\n",
    "thesis_df = pd.DataFrame(columns = ['keywords', 'id_thesis'])\n",
    "\n",
    "for element in input_list : \n",
    "  print(\"\\n-------------------------------------------------\")\n",
    "  print(\"Element : \" + element)\n",
    "\n",
    "  # Recover previous researches results \n",
    "  try : \n",
    "    seen_df = pd.read_csv(element + \".csv\")[\"seen_id_thesis\"].tolist()\n",
    "    print(\"File from previous researches found.\\n\")\n",
    "\n",
    "    print(\"Number of thesis already seen in preceent researches : \" + str(len(seen_df)))\n",
    "  except : \n",
    "    print(\"This request has no precedent.\\n\")\n",
    "    seen_df = []\n",
    "  \n",
    "  # Verifying number of results\n",
    "  try : \n",
    "    number_results = scraping_number_results(\"https://www.theses.fr/?q=\" + element)\n",
    "    print(number_results, \" results for element : \", element)\n",
    "  except : \n",
    "    number_results = 0\n",
    "    print(\"No results for this element : \" + str(element))\n",
    "\n",
    "  # Extract results\n",
    "  if (number_results > 0) : \n",
    "    try : \n",
    "      # scrap_results = result_scraping(number_results, \"https://www.theses.fr/?q=\" + element + \"&fq=dateSoutenance:[1965-01-01T23:59:59Z%2BTO%2B\"\"extract_['transformation+chimique', 'pathologie+digestive+numerique']_2023-06-08.xlsx\"+ \n",
    "      # datetime.now().strftime(\"%Y-%m-%d\") + \"T\" + datetime.now().strftime(\"%H:%M:%S\") + \"Z\" + \n",
    "      # \"]&checkedfacets=&start={}&sort=none&status=&access=&prevision=&filtrepersonne=&zone1=titreRAs&val1=&op1=AND&zone2=auteurs&val2=&op2=AND&zone3=etabSoutenances&val3=&op3=AND&zone4=dateSoutenance&val4a=&val4b=&type=&lng=fr/&checkedfacets=&format=csv\")\n",
    "      \n",
    "      scrap_results = result_scraping( number_results,\n",
    "        \"https://www.theses.fr/?q=\"+ element + \n",
    "        \"&fq=dateSoutenance:[1965-01-01T23:59:59Z%2BTO%2B2023-12-31T23:59:59Z]&checkedfacets=&start={}&sort=none&status=&access=&prevision=&filtrepersonne=&zone1=titreRAs&val1=&op1=AND&zone2=auteurs&val2=&op2=AND&zone3=etabSoutenances&val3=&op3=AND&zone4=dateSoutenance&val4a=&val4b=&type=&lng=fr/&checkedfacets=&format=csv\"  )\n",
    "      \n",
    "      # if element == \"transformation+chimique\" :\n",
    "      #  df.to_csv(\"transformation+chimique_raw.csv\")\n",
    "      \n",
    "      scrap_results = scrap_results[[\"Statut\", \"Identifiant de la these\", \"Accessible en ligne\", \"Titre\", \"Auteur\", \"Directeur de these (nom prenom)\", \"Etablissement de soutenance\", \"Discipline\"]]\n",
    "      scrap_results[\"Identifiant de la these\"] = scrap_results[\"Identifiant de la these\"].apply(lambda x : \"https://www.theses.fr/\" + x)\n",
    "      \n",
    "      seen_precedent_researches = 0\n",
    "      illegal_thesis = 0\n",
    "      redundant_thesis = 0\n",
    "\n",
    "      for id_thesis in scrap_results[\"Identifiant de la these\"] : \n",
    "        # print(\"ID thesis evaluate : \" + id_thesis)\n",
    "        \n",
    "        if id_thesis in seen_df : \n",
    "          seen_precedent_researches += 1\n",
    "        else : \n",
    "          seen_df = seen_df + [id_thesis]\n",
    "          if id_thesis in illegal_url_list :\n",
    "            illegal_thesis += 1\n",
    "          else : \n",
    "            if id_thesis in thesis_df_temp :\n",
    "              redundant_thesis += 1\n",
    "            else : \n",
    "              thesis_df_temp = thesis_df_temp + [id_thesis]\n",
    "      \n",
    "      # Save already seen thesis\n",
    "      pd.DataFrame(seen_df, columns = [\"seen_id_thesis\"]).to_csv(str(element) + \".csv\", index = False)\n",
    "      \n",
    "      # unser search results\n",
    "      thesis_df = pd.concat([thesis_df,  \n",
    "                            pd.DataFrame({'keywords':element, 'id_thesis':thesis_df_temp}).merge(scrap_results.rename(columns = {\"Identifiant de la these\":\"id_thesis\"}), \n",
    "                                                                                on=\"id_thesis\", \n",
    "                                                                                how = \"left\")],\n",
    "                            ignore_index = True)\n",
    "      print(\"Thesis already seen in precedent researches : \" + str(seen_precedent_researches))\n",
    "      print(\"duplicate : \" + str(redundant_thesis))\n",
    "      print(\"Disallow thesis : \" + str(illegal_thesis))\n",
    "\n",
    "      print(\"Extraction from \" + str(element) + \" finished! \")\n",
    "\n",
    "    except : \n",
    "      print(\"Error in element : \", element)  \n",
    "\n",
    "# Save new thesis extracted\n",
    "thesis_df.to_excel(\"extract_\" + str(input_list) + \"_\" + str(datetime.now())[:10] + \".xlsx\", sheet_name = \"extraction\", index = False)\n",
    "# thesis_df.to_csv(\"extract_\" + str(input_list) + \"_\" + str(datetime.now())[:10] + \".csv\", index = False)\n",
    "\n",
    "print(\"\\n---------------------------\\nExecution finished !\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "df05c270741478968602ac2f9dc38780876460d38450bd42c3ad03db9d6ea779"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
