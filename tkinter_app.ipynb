{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "from tkinter import messagebox\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import openpyxl\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for tkinter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input_parsing(input_user) : \n",
    "    input_list = input_user.split(\";\")\n",
    "    mask = []\n",
    "\n",
    "    for count, element in enumerate(input_list) : \n",
    "        print(\"element n°\", count+1, \" on \", len(input_list))\n",
    "        element = re.sub(' +', ' ', element) # Delete multiple spaces\n",
    "        element = re.sub('^ +', '', element) # Delete spaces before reseach\n",
    "        element = re.sub(' +$', '', element) # Delete spaces after reseach\n",
    "        element = re.sub(' ', '+', element)\n",
    "        element = unidecode(element) # delete accent\n",
    "        \n",
    "        print(\"element : '\" + element + \"'\")\n",
    "        \n",
    "        if len(element) == 0 :\n",
    "            print(\"1 element deleted because containing nothing\")\n",
    "        else : \n",
    "            mask = mask + [element]\n",
    "\n",
    "    input_list = mask\n",
    "\n",
    "    # Verifying output\n",
    "    # print(\"Liste finale : \")\n",
    "    print(\"Final list of elements: \",str(input_list))\n",
    "\n",
    "    return input_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract results.\n",
    "# theses.fr allow to export only 1000 results in the same file, so it's necessary to make multiple exports and stack them.\n",
    "\n",
    "def scraping_number_results(url_short) :\n",
    "  url_short = url_short\n",
    "\n",
    "  # Number of results\n",
    "  html = requests.get(url_short)\n",
    "  soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "  number_results = int(soup.find(\"div\", attrs={\"id\":\"resumR\"}).find(\"span\", attrs={\"id\":\"sNbRes\"}).text)\n",
    "\n",
    "  return number_results\n",
    "\n",
    "def result_scraping(n_res, url, element) :\n",
    "  url = url\n",
    "  number_results = n_res\n",
    "  \n",
    "  # Variables\n",
    "  df_temp = pd.DataFrame()\n",
    "  definitive_df  = pd.DataFrame()\n",
    "\n",
    "  # Loop extraction\n",
    "  start = 0\n",
    "  number_results_loop = number_results\n",
    "\n",
    "  while number_results_loop >= 0 :\n",
    "    number_results_loop -= 1000\n",
    "    # print(\"url : \" + str(url.format(start)))\n",
    "    print(\"For element : \" + element + \", extraction from \" + str(start) + \" to \" + str(min(number_results, start+1000)))\n",
    "    df_temp = pd.read_csv(url.format(start), sep = \";\")\n",
    "\n",
    "    definitive_df = pd.concat([definitive_df, df_temp], ignore_index = True)\n",
    "    \n",
    "    start += 1000\n",
    "\n",
    "  # inital : df = pd.read_csv(url)\n",
    "  # pb : si plus de 1000 résultats, csv ne charge que les 1000 premeirs résultats\n",
    "  return definitive_df\n",
    "\n",
    "# input_list = [\"transformation+chimique\", \"pathologie+digestive+numerique\"] # input list for testing\n",
    "\n",
    "def core_scrap(input_list) : \n",
    "  thesis_df_temp = []\n",
    "  thesis_df = pd.DataFrame(columns = ['keywords', 'id_thesis'])\n",
    "\n",
    "  robots_df = pd.read_csv(\"https://www.theses.fr/robots.txt\", sep = \": \").rename(columns = {\"User-agent\":\"col\", \"*\":\"id_thesis\"})\n",
    "  robots_df = robots_df[ (robots_df[\"col\"] != \"Crawl-delay\") & (robots_df[\"col\"] != \"Sitemap\") ] # Delete Site map and Crawl-delay rows\n",
    "  illegal_url_list = robots_df.id_thesis.apply(lambda x: \"https://www.theses.fr\"+x).tolist() # List of disallow URL\n",
    "\n",
    "  for element in input_list : \n",
    "    print(\"\\n-------------------------------------------------\")\n",
    "    print(\"Element : \" + element)\n",
    "\n",
    "    # Recover previous researches results \n",
    "    try : \n",
    "      seen_df = pd.read_csv(element + \".csv\")[\"seen_id_thesis\"].tolist()\n",
    "      print(\"File from previous researches found.\\n\")\n",
    "\n",
    "      print(\"Number of thesis already seen in preceent researches : \" + str(len(seen_df)))\n",
    "    except : \n",
    "      print(\"This request has no precedent.\\n\")\n",
    "      seen_df = []\n",
    "    \n",
    "    # Verifying number of results\n",
    "    try : \n",
    "      number_results = scraping_number_results(\"https://www.theses.fr/?q=\" + element)\n",
    "      print(number_results, \" results for element : \", element)\n",
    "    except : \n",
    "      number_results = 0\n",
    "      print(\"No results for this element : \" + str(element))\n",
    "\n",
    "    # Extract results\n",
    "    if (number_results > 0) : \n",
    "      try : \n",
    "        # scrap_results = result_scraping(number_results, \"https://www.theses.fr/?q=\" + element + \"&fq=dateSoutenance:[1965-01-01T23:59:59Z%2BTO%2B\"\"extract_['transformation+chimique', 'pathologie+digestive+numerique']_2023-06-08.xlsx\"+ \n",
    "        # datetime.now().strftime(\"%Y-%m-%d\") + \"T\" + datetime.now().strftime(\"%H:%M:%S\") + \"Z\" + \n",
    "        # \"]&checkedfacets=&start={}&sort=none&status=&access=&prevision=&filtrepersonne=&zone1=titreRAs&val1=&op1=AND&zone2=auteurs&val2=&op2=AND&zone3=etabSoutenances&val3=&op3=AND&zone4=dateSoutenance&val4a=&val4b=&type=&lng=fr/&checkedfacets=&format=csv\")\n",
    "        \n",
    "        scrap_results = result_scraping( number_results,\n",
    "          \"https://www.theses.fr/?q=\"+ element + \n",
    "          \"&fq=dateSoutenance:[1965-01-01T23:59:59Z%2BTO%2B2023-12-31T23:59:59Z]&checkedfacets=&start={}&sort=none&status=&access=&prevision=&filtrepersonne=&zone1=titreRAs&val1=&op1=AND&zone2=auteurs&val2=&op2=AND&zone3=etabSoutenances&val3=&op3=AND&zone4=dateSoutenance&val4a=&val4b=&type=&lng=fr/&checkedfacets=&format=csv\", \n",
    "          element  )\n",
    "        \n",
    "        scrap_results = scrap_results[[\"Statut\", \"Identifiant de la these\", \"Accessible en ligne\", \"Titre\", \"Auteur\", \"Directeur de these (nom prenom)\", \"Etablissement de soutenance\", \"Discipline\"]]\n",
    "        scrap_results[\"Identifiant de la these\"] = scrap_results[\"Identifiant de la these\"].apply(lambda x : \"https://www.theses.fr/\" + x)\n",
    "        \n",
    "        seen_precedent_researches = 0\n",
    "        illegal_thesis = 0\n",
    "        redundant_thesis = 0\n",
    "        \n",
    "        for id_thesis in scrap_results[\"Identifiant de la these\"] : \n",
    "          # print(\"ID thesis evaluate : \" + id_thesis)\n",
    "          \n",
    "          if id_thesis in seen_df : \n",
    "            seen_precedent_researches += 1\n",
    "          else : \n",
    "            seen_df = seen_df + [id_thesis]\n",
    "            if id_thesis in illegal_url_list :\n",
    "              illegal_thesis += 1\n",
    "            else : \n",
    "              if id_thesis in thesis_df_temp :\n",
    "                redundant_thesis += 1\n",
    "              else : \n",
    "                thesis_df_temp = thesis_df_temp + [id_thesis]\n",
    "        \n",
    "        # Save already seen thesis\n",
    "        pd.DataFrame(seen_df, columns = [\"seen_id_thesis\"]).to_csv(str(element) + \".csv\", index = False)\n",
    "        \n",
    "        # unser search results\n",
    "        thesis_df = pd.concat([thesis_df,  \n",
    "                              pd.DataFrame({'keywords':element, 'id_thesis':thesis_df_temp}).merge(scrap_results.rename(columns = {\"Identifiant de la these\":\"id_thesis\"}), \n",
    "                                                                                  on=\"id_thesis\", \n",
    "                                                                                  how = \"left\")],\n",
    "                              ignore_index = True)\n",
    "        print(\"Thesis already seen in precedent researches : \" + str(seen_precedent_researches))\n",
    "        print(\"duplicate : \" + str(redundant_thesis))\n",
    "        print(\"Disallow thesis : \" + str(illegal_thesis))\n",
    "        print(\"Extraction from \" + str(element) + \" finished! \")\n",
    "\n",
    "      except : \n",
    "        print(\"Error in element : \", element)  \n",
    "\n",
    "  # Save new thesis extracted\n",
    "  thesis_df.to_excel(\"extract_\" + str(input_list) + \"_\" + str(datetime.now())[:19].replace(\":\", \"-\") + \".xlsx\", sheet_name = \"extraction\", index = False)\n",
    "  # thesis_df.to_csv(\"extract_\" + str(input_list) + \"_\" + str(datetime.now())[:10] + \".csv\", index = False)\n",
    "\n",
    "  print(\"\\n---------------------------\\nExecution finished !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tkinter window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_text  = \"Cette application a pour but de vous aider à faire votre veille stratégique sur le site https://www.theses.fr. \\n\\n\" +\\\n",
    "\"Pour celà, voici les étapes : \\n\" +\\\n",
    "\"1. Sélectionner les mots-clés que vous souhaitez rechercher. \\n Vous pouvez entrez plusieurs mots-clés si vous les séparez par un point-virgule (;).\\n\" +\\\n",
    "\"2. Cliquez sur le bouton 'Rechercher' pour lancer la recherche. \\n\\n\" +\\\n",
    "\"Les tips : \\n\" +\\\n",
    "\"- les theses récupérées lors d'anciennes recherches ne ressortiront pas dans la recherche lancée et les futures recherches.\\n\" +\\\n",
    "\"Vous pouvez supprimer l'historique des recherches en entrant les mots-clés souhaitées et en cliquant sur 'Supprimer l'historique'. \\n\" +\\\n",
    "\"ATTENTION : pour supprimer l'historique, 1 seul mot clé à la fois !\\n\" +\\\n",
    "\"- pour quitter l'application, cliquez sur le bouton 'Quitter'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element n° 1  on  1\n",
      "element : 'transformation+chimique+:+pathologie+digitale'\n",
      "Final list of elements:  ['transformation+chimique+:+pathologie+digitale']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdvd270\\AppData\\Local\\Temp\\ipykernel_13492\\2359913358.py:46: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  robots_df = pd.read_csv(\"https://www.theses.fr/robots.txt\", sep = \": \").rename(columns = {\"User-agent\":\"col\", \"*\":\"id_thesis\"})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------\n",
      "Element : transformation+chimique+:+pathologie+digitale\n",
      "This request has no precedent.\n",
      "\n",
      "303  results for element :  transformation+chimique+:+pathologie+digitale\n",
      "For element : transformation+chimique+:+pathologie+digitale, extraction from 0 to 303\n",
      "Thesis already seen in precedent researches : 0\n",
      "duplicate : 0\n",
      "Disallow thesis : 0\n",
      "Extraction from transformation+chimique+:+pathologie+digitale finished! \n",
      "\n",
      "---------------------------\n",
      "Execution finished !\n",
      "element n° 1  on  2\n",
      "element : 'transformation+chimique'\n",
      "element n° 2  on  2\n",
      "element : 'pathologie+digitale'\n",
      "Final list of elements:  ['transformation+chimique', 'pathologie+digitale']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdvd270\\AppData\\Local\\Temp\\ipykernel_13492\\2359913358.py:46: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  robots_df = pd.read_csv(\"https://www.theses.fr/robots.txt\", sep = \": \").rename(columns = {\"User-agent\":\"col\", \"*\":\"id_thesis\"})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------\n",
      "Element : transformation+chimique\n",
      "File from previous researches found.\n",
      "\n",
      "Number of thesis already seen in preceent researches : 20905\n",
      "20905  results for element :  transformation+chimique\n",
      "For element : transformation+chimique, extraction from 0 to 1000\n",
      "For element : transformation+chimique, extraction from 1000 to 2000\n",
      "For element : transformation+chimique, extraction from 2000 to 3000\n",
      "For element : transformation+chimique, extraction from 3000 to 4000\n",
      "For element : transformation+chimique, extraction from 4000 to 5000\n",
      "For element : transformation+chimique, extraction from 5000 to 6000\n",
      "For element : transformation+chimique, extraction from 6000 to 7000\n",
      "For element : transformation+chimique, extraction from 7000 to 8000\n",
      "For element : transformation+chimique, extraction from 8000 to 9000\n",
      "For element : transformation+chimique, extraction from 9000 to 10000\n",
      "For element : transformation+chimique, extraction from 10000 to 11000\n",
      "For element : transformation+chimique, extraction from 11000 to 12000\n",
      "For element : transformation+chimique, extraction from 12000 to 13000\n",
      "For element : transformation+chimique, extraction from 13000 to 14000\n",
      "For element : transformation+chimique, extraction from 14000 to 15000\n",
      "For element : transformation+chimique, extraction from 15000 to 16000\n",
      "For element : transformation+chimique, extraction from 16000 to 17000\n",
      "For element : transformation+chimique, extraction from 17000 to 18000\n",
      "For element : transformation+chimique, extraction from 18000 to 19000\n",
      "For element : transformation+chimique, extraction from 19000 to 20000\n",
      "For element : transformation+chimique, extraction from 20000 to 20905\n",
      "Thesis already seen in precedent researches : 20905\n",
      "duplicate : 0\n",
      "Disallow thesis : 0\n",
      "Extraction from transformation+chimique finished! \n",
      "\n",
      "-------------------------------------------------\n",
      "Element : pathologie+digitale\n",
      "File from previous researches found.\n",
      "\n",
      "Number of thesis already seen in preceent researches : 823\n",
      "823  results for element :  pathologie+digitale\n",
      "For element : pathologie+digitale, extraction from 0 to 823\n",
      "Thesis already seen in precedent researches : 823\n",
      "duplicate : 0\n",
      "Disallow thesis : 0\n",
      "Extraction from pathologie+digitale finished! \n",
      "\n",
      "---------------------------\n",
      "Execution finished !\n",
      "element n° 1  on  1\n",
      "element : 'pathologie+digitale'\n",
      "Final list of elements:  ['pathologie+digitale']\n",
      "delete hisotric launch\n",
      "pathologie+digitale\n",
      "found and delete\n",
      "element n° 1  on  1\n",
      "element : 'transformation+chimique'\n",
      "Final list of elements:  ['transformation+chimique']\n",
      "delete hisotric launch\n",
      "transformation+chimique\n",
      "found and delete\n",
      "delete hisotric launch\n",
      "transformation+chimique\n",
      "unfound\n"
     ]
    }
   ],
   "source": [
    "root = tk.Tk()\n",
    "root.title(\"Veille stratégique theses.fr\")\n",
    "root.geometry('1000x500')\n",
    "\n",
    "# ---- Etape 0 : Présentation\n",
    "lbl_title_0 = Label(root, text = \"Etape 0 : Présentation du projet\", justify = LEFT)\n",
    "lbl_title_0.config(font=(\"Courier\", 12))\n",
    "lbl_title_0.grid(column = 0, row = 0)\n",
    "\n",
    "# ---- Explanation texts\n",
    "lbl = Label(root, text = explanation_text, justify = LEFT)\n",
    "lbl.grid(column = 0, row = 1, columnspan = 4)\n",
    "\n",
    "# ---- Etape 1 : les mots clés\n",
    "lbl_title_1 = Label(root, text = \"Etape 1 : saisie des mots de recherche\", justify = LEFT)\n",
    "lbl_title_1.config(font=(\"Courier\", 12))\n",
    "lbl_title_1.grid(column = 0, row = 2)\n",
    "\n",
    "# --- input_user entry zone\n",
    "input_user = Entry(root, width = 100)\n",
    "input_user.grid(column = 0, row = 3, sticky=\"w\")\n",
    "\n",
    "def clicked():\n",
    "   user_input = user_input_parsing(input_user.get())\n",
    "   with open(\"user_input.pickle\", \"wb\") as fp:   #Pickling\n",
    "      pickle.dump(user_input, fp)\n",
    "\n",
    "   res = \"Votre saisie : \" + input_user.get()\n",
    "   lbl_user_input.configure(text = res)\n",
    "\n",
    "btn = Button(root, text = \"Valider votre saisie\" , command=clicked)\n",
    "btn.grid(column = 2, row = 3)\n",
    "\n",
    "# ---- Print user input\n",
    "lbl_user_input = Label(root, text=\"\", justify = LEFT)\n",
    "lbl_user_input.grid(column=0, row = 4)\n",
    "\n",
    "# ---- Choix des actions\n",
    "lbl_title_2 = Label(root, text = \"Etape 2 : les actions\", justify = LEFT)\n",
    "lbl_title_2.config(font=(\"Courier\", 12))\n",
    "lbl_title_2.grid(column = 0, row = 5)\n",
    "\n",
    "# ---- lancer la recherche\n",
    "def launch_scrap() :\n",
    "   with open(\"user_input.pickle\", \"rb\") as fp:   # Unpickling\n",
    "      input_list = pickle.load(fp)\n",
    "\n",
    "   core_scrap(input_list)\n",
    "\n",
    "btn_scrap = Button(root, text = \"lancer le scraping\" , command=launch_scrap)\n",
    "btn_scrap.grid(column = 0, row = 6)\n",
    "\n",
    "# ---- Supprimer son historique\n",
    "def delete_historic() :\n",
    "   print(\"delete hisotric launch\")\n",
    "   \n",
    "   with open(\"user_input.pickle\", \"rb\") as fp:   # Unpickling\n",
    "      user_input = pickle.load(fp)\n",
    "\n",
    "   for element in user_input :\n",
    "      print(element)\n",
    "      try :\n",
    "         os.remove(element + \".csv\")\n",
    "         print(\"found and delete\")\n",
    "      except : \n",
    "         print(\"unfound\")\n",
    "         \n",
    "btn_hist = Button(root, text = \"Supprimer historique\" , command=delete_historic)\n",
    "btn_hist.grid(column = 2, row = 6)\n",
    "\n",
    "# ---- Conclusion\n",
    "lbl_title_3 = Label(root, text = \"Etape 3 : avant de partir\", justify = LEFT)\n",
    "lbl_title_3.config(font=(\"Courier\", 12))\n",
    "lbl_title_3.grid(column = 0, row = 7)\n",
    "\n",
    "lbl_con = Label(root, text = \"Quittez la fenetre pour quitter le programme.\\n\" + \n",
    "\"Les résultats des extractions se trouveront dans le dossier ou se trouve le script python.\", justify = LEFT)\n",
    "lbl_con.grid(column = 0, row = 8)\n",
    "\n",
    "# ---- Main loop\n",
    "root.mainloop()\n",
    "\n",
    "os.remove(\"user_input.pickle\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df05c270741478968602ac2f9dc38780876460d38450bd42c3ad03db9d6ea779"
  },
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
